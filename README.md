# RAG Chatbot

This project implements a **Retrieval-Augmented Generation (RAG)** chatbot that retrieves context from a ChromaDB vector database and generates responses using the Groq API (`llama-3.3-70b-versatile`) or a fallback `distilgpt2` model. The chatbot supports interactive command-line conversations and API-based querying via FastAPI, with embeddings generated by SentenceTransformer (`all-MiniLM-L6-v2`).The application is dockerized and hosted on Docker Hub for easy deployment.

## Features
- **RAG Pipeline**: Combines retrieval from ChromaDB with response generation via Groq API or local LLM.
- **Persistent Vector Storage**: Stores question-response pairs in ChromaDB (`chromadb` directory).
- **FastAPI Server**: Exposes a `/query` endpoint for API-based interaction.
- **Interactive CLI**: Command-line interface for local testing (`chatbot.py`).
- **Modular Design**: Separates concerns into `app.py`, `chatbot.py`, `embedding.py`, `llms.py`, `rag.py`, and `vectordb.py`.
- **Docker Support:** Containerized with Dockerfile and available on Docker Hub (imsnto/rag-chatbot:latest).
- **Exit Command**: Type `exit` in CLI to end the conversation.
- **Environment Variables**: Securely stores Groq API key in `.env`.

## Prerequisites
- **Python 3.8+**
- **Docker** (for containerized deployment, see Docker Installation)
- **Groq API Key**: Obtain from [console.groq.com](https://console.groq.com/keys).
- **Dependencies**: Listed in `requirements.txt` (e.g., `chromadb`, `sentence-transformers`, `fastapi`).

## Docker Installation

To run the chatbot using Docker, install Docker on your system.

### Ubuntu

1. Update package index:

   ```bash
   sudo apt update
   sudo apt upgrade -y
   ```
2. Install Docker:

   ```bash
   sudo apt install -y docker.io docker-compose
   sudo usermod -aG docker $USER
   ```
3. Log out and back in, then verify:

   ```bash
   docker --version
   docker run hello-world
   ```

### Windows

1. Download and install Docker Desktop for Windows.
2. Enable WSL 2 (Windows Subsystem for Linux) if prompted:

   ```powershell
   wsl --install
   ```
3. Launch Docker Desktop and verify:

   ```powershell
   docker --version
   docker run hello-world
   ```

### macOS

1. Download and install Docker Desktop for macOS.
2. Launch Docker Desktop and verify:

   ```bash
   docker --version
   docker run hello-world
   ```

## Running with Docker

Run the chatbot using the pre-built Docker image from Docker Hub.

1. **Configure Groq API Key**:

   - Create a `.env` file in your working directory:

     ```env
     GROQ_API_KEY=your_groq_api_key_here
     ```
   - Obtain the key from console.groq.com.

2. **Run the Docker Container**:

   ```bash
   docker run -d -p 8000:8000 --env-file .env -v $(pwd)/chromadb:/app/chromadb imsnto/rag-chatbot:latest
   ```

   - `-d`: Runs in detached mode.
   - `-p 8000:8000`: Maps port 8000 for API access.
   - `--env-file .env`: Loads `GROQ_API_KEY`.
   - `-v $(pwd)/chromadb:/app/chromadb`: Persists ChromaDB data.
   - Creates a `chromadb` folder if it doesn’t exist.

3. **Verify**:

   - Check container status:

     ```bash
     docker ps
     ```
   - View logs:

     ```bash
     docker logs <container_id>
     ```
     - Look for:

       ```
       INFO:__main__:Collection count: 5
       INFO:__main__:Files in /app/chromadb: ['chroma.sqlite3']
       ```
   - Test API:

     ```bash
     curl -X POST http://localhost:8000/query -H "Content-Type: application/json" -d '{"query": "What is AI?"}'
     ```
     - Expected:

       ```json
       {"response": "AI is a field of computer science that develops systems capable of reasoning, learning, and decision-making."}
       ```

4. **Stop the Container**:

   ```bash
   docker stop <container_id>
   ```

## Local Development (Optional)
For development or running the CLI without Docker, set up locally.

## Installation
1. **Clone the Repository**:
   ```bash
   git clone https://github.com/imsnto/chatbot.git
   cd chatbot
   ```

2. **Set Up a Virtual Environment**:
   ```bash
   python -m venv .venv
   source .venv/bin/activate  # On Windows: .venv\Scripts\activate
   ```

3. **Install Dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

4. **Install and Configure Ollama**:
   - Install Ollama: Follow instructions at [ollama.ai](https://ollama.ai/).
   - Pull the `qwen2.5:0.5b` model:
     ```bash
     ollama pull qwen2.5:0.5b
     ```
   - Ensure the Ollama server is running:
     ```bash
     ollama serve
     ```
5. **Configure HF and Groq API Key:**
   - Create a `.env` file in the project root:
   ```bash
   GROQ_API_KEY=your_groq_api_key_here
   HF_TOKEN=your huggingface token
   ```
   - Ensure `.env` is listed in .gitignore.


## Usage
   **Interactive CLI**
1. Run the chatbot script:
   ```bash
   python main.py
   ```
   or
   ```bash
   python chatbot.py
   ```

2. Interact with the chatbot:
   - Enter your message at the `You: ` prompt.
   - Type `exit` to quit.
   - Example interaction:
     ```
     You: Hello, how are you?
     Bot: I'm doing great, thanks for asking! How about you?
     You: I'm learning LangChain.
     Bot: Nice! LangChain is awesome for building AI apps. Want tips?
     You: exit
     ```
**FastAPI Server**
1. Start the server:
   ```bash
   python app.py
   ```

## Project Structure
```
├── .env                    # Environment variables (GROQ_API_KEY)
├── .gitignore              # Ignores venv/, __pycache__/, .env
├── README.md               # Project documentation
├── Dockerfile              # Docker image configuration
├── docker-compose.yml      # Docker Compose for local testing
├── requirements.txt        # Python dependencies
├── app.py                  # FastAPI server for API-based querying
├── chatbot.py              # Interactive CLI for local testing
├── embedding.py            # SentenceTransformer embedding model
├── llms.py                 # Groq API and distilgpt2 LLM setup
├── rag.py                  # RAG pipeline (retrieval + generation)
├── vectordb.py             # ChromaDB setup and sample data
└── chromadb/               # Persistent ChromaDB storage        
```

## Notes
- Ensure the Ollama server is running before executing `main.py`.
- The `.gitignore` file should include `.venv/` and other irrelevant files (e.g., `__pycache__/`, `.env`).
- The chatbot maintains conversation history in memory during the session. History is reset when the script exits.
- **Groq API:** Requires an active internet connection and valid API key. Rate limits apply (e.g., 30 requests/minute for free tier).
- **Fallback LLM:** Uses distilgpt2 locally if Groq API is unavailable (commented in llms.py).
- **Docker**:
  - Ensure Docker Desktop is running (Windows/macOS).
  - Use `docker-compose.yml` for local testing with volume mounts.



## Contributing
1. Fork the repository.
2. Create a feature branch:
   ```bash
   git checkout -b feature/your-feature
   ```
3. Commit changes:
   ```bash
   git commit -m "Add your feature"
   ```
4. Push to the branch:
   ```bash
   git push origin feature/your-feature
   ```
5. Open a pull request.

## License
This project is licensed under the MIT. See the [LICENSE](LICENSE) file for details.

## Resources
- [LangChain Documentation](https://python.langchain.com/docs/)
- [Ollama GitHub](https://github.com/ollama/ollama)
- [qwen2.5 Model](https://ollama.ai/library/qwen2.5)
